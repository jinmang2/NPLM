{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets.txt', 'r', encoding='utf-8') as f:\n",
    "    datasets = f.readlines()\n",
    "    datasets = ''.join(datasets).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발 없는 말이 천리 간다',\n",
       " '모난 돌이 정 맞는다',\n",
       " '고래 싸움에 새우 등 터진다',\n",
       " '끝 부러진 송곳',\n",
       " '들으면 병이요  안 들으면 약이다',\n",
       " '봄눈 녹듯 한다',\n",
       " '열 손가락을 깨물어서 안 아픈 손가락 없다',\n",
       " '첫딸은 살림 밑천이다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[::40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = \" \".join(datasets).split()\n",
    "word_list = list(set(word_list))\n",
    "unk_token = '<UNK>'\n",
    "word_list.append(unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "number_dict = {i:w for i, w in enumerate(word_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE  : 1008\n",
      "EMBED_SIZE  : 30\n",
      "HIDDEN_SIZE : 32\n",
      "NGRAM_SIZE  : 3\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_dict)\n",
    "EMBED_SIZE = 30\n",
    "HIDDEN_SIZE = 32\n",
    "NGRAM_SIZE = 3\n",
    "\n",
    "print('VOCAB_SIZE  :', VOCAB_SIZE)\n",
    "print('EMBED_SIZE  :', EMBED_SIZE)\n",
    "print('HIDDEN_SIZE :', HIDDEN_SIZE)\n",
    "print('NGRAM_SIZE  :', NGRAM_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sen in enumerate(datasets):\n",
    "    word_list = sen.split()\n",
    "    if len(word_list) < NGRAM_SIZE:\n",
    "        word_list.insert(0, unk_token)\n",
    "        sen = ' '.join(word_list)\n",
    "    datasets[i] = sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [sen.replace('  ', ' ') for sen in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for sen in datasets:\n",
    "    text = sen.strip().split(' ')\n",
    "    for i in range(len(text) - (NGRAM_SIZE - 1)):\n",
    "        train_data.append(' '.join(text[i:i+NGRAM_SIZE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_generator(sentences, batch_size):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    batch_ix = 0\n",
    "    for sentence in sentences[batch_ix:]:\n",
    "        if batch_ix == batch_size:\n",
    "            yield input_batch, target_batch\n",
    "            input_batch = []\n",
    "            target_batch = []\n",
    "            batch_ix = 0\n",
    "        words = sentence.split()\n",
    "        input_ = [word_dict[n] for n in words[:-1]]\n",
    "        target_ = word_dict[words[-1]]\n",
    "        input_batch.append(input_)\n",
    "        target_batch.append(target_)\n",
    "        batch_ix += 1\n",
    "        \n",
    "train_generator = make_batch_generator(train_data, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input_ = [word_dict[n] for n in word[:-1]]\n",
    "        target_ = word_dict[word[-1]]\n",
    "        \n",
    "        input_batch.append(input_)\n",
    "        target_batch.append(target_)\n",
    "        \n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32\n",
      "32 64\n",
      "64 96\n",
      "96 128\n",
      "128 160\n",
      "160 192\n",
      "192 224\n",
      "224 256\n",
      "256 288\n",
      "288 320\n",
      "320 352\n",
      "352 384\n",
      "384 416\n",
      "416 448\n",
      "448 480\n",
      "480 512\n",
      "512 544\n",
      "544 576\n",
      "576 608\n",
      "608 640\n",
      "640 672\n",
      "672 704\n",
      "704 736\n",
      "736 768\n",
      "768 800\n",
      "800 832\n",
      "832 864\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target_batch) // 32 + 1):\n",
    "    print(i * 32, (i+1) * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = Variable(torch.LongTensor(input_batch[:32]))\n",
    "target_batch = Variable(torch.LongTensor(target_batch[:32]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[359, 424],\n",
       "        [424, 584],\n",
       "        [584, 893],\n",
       "        [813, 947],\n",
       "        [947, 583],\n",
       "        [612, 900],\n",
       "        [900, 474],\n",
       "        [474, 809],\n",
       "        [809, 354],\n",
       "        [354, 277],\n",
       "        [277,   0],\n",
       "        [  0,  24],\n",
       "        [ 24, 264],\n",
       "        [264, 713],\n",
       "        [479, 767],\n",
       "        [767, 381],\n",
       "        [381, 335],\n",
       "        [335, 903],\n",
       "        [903,  21],\n",
       "        [ 21, 939],\n",
       "        [939, 873],\n",
       "        [873, 360],\n",
       "        [360, 131],\n",
       "        [942,  65],\n",
       "        [ 65, 339],\n",
       "        [339, 997],\n",
       "        [997, 330],\n",
       "        [394, 133],\n",
       "        [564, 883],\n",
       "        [883,  92],\n",
       "        [564, 688],\n",
       "        [688, 953]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([584, 893, 495, 583, 992, 474, 809, 354, 277,   0,  24, 264, 713, 252,\n",
       "        381, 335, 903,  21, 939, 873, 360, 131, 806, 339, 997, 330, 732, 685,\n",
       "         92, 205, 953, 888])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_batch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_batch = target_batch.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = nn.Embedding(VOCAB_SIZE, EMBED_SIZE).cuda()\n",
    "H = nn.Parameter(torch.randn((NGRAM_SIZE - 1) * EMBED_SIZE, HIDDEN_SIZE).type(dtype)).cuda()\n",
    "W = nn.Parameter(torch.randn((NGRAM_SIZE - 1) * EMBED_SIZE, VOCAB_SIZE).type(dtype)).cuda()\n",
    "d = nn.Parameter(torch.randn(HIDDEN_SIZE).type(dtype)).cuda()\n",
    "U = nn.Parameter(torch.randn(HIDDEN_SIZE, VOCAB_SIZE).type(dtype)).cuda()\n",
    "b = nn.Parameter(torch.randn(VOCAB_SIZE).type(dtype)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1008, 30])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(C.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = C(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 30])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.view(-1, (NGRAM_SIZE-1) * EMBED_SIZE).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.view(32, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.view(32, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = torch.tanh(d + torch.mm(X, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9429, -1.0000, -0.9998,  ...,  0.9762, -0.9999,  0.9827],\n",
       "        [-0.5350,  1.0000,  1.0000,  ..., -0.8565,  1.0000,  0.9929],\n",
       "        [ 0.9966, -1.0000,  1.0000,  ..., -1.0000, -1.0000,  0.9988],\n",
       "        ...,\n",
       "        [-0.3040,  1.0000, -0.9997,  ..., -1.0000,  0.9987,  0.4833],\n",
       "        [-1.0000, -0.9961, -1.0000,  ...,  0.9943,  0.9964, -0.9790],\n",
       "        [ 0.8429, -0.6702, -0.8870,  ..., -1.0000, -0.9822, -0.9526]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = b + torch.mm(X, W) + torch.mm(tanh, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.0588,  -8.7777,  16.5063,  ...,  11.1187,   7.6718,   7.4594],\n",
       "        [ -7.8521,   7.7280,   9.7059,  ...,  -7.6543, -17.2225,   3.7947],\n",
       "        [ 13.5479,  -1.4848,  -0.1105,  ..., -10.1636,  -9.1711,   9.1920],\n",
       "        ...,\n",
       "        [ 10.6150,  -2.1153,  -3.6730,  ...,  -6.7203,  -6.2340,   5.3053],\n",
       "        [  8.2681,  14.5065,  11.0054,  ..., -10.2024,  14.5706,  17.0870],\n",
       "        [ -1.3668,   3.6505, -12.9709,  ...,  -5.7174,  -2.6522,   5.4203]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1008])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPLM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 VOCAB_SIZE,\n",
    "                 EMBED_SIZE=30,\n",
    "                 HIDDEN_SIZE=32,\n",
    "                 NGRAM_SIZE=2):\n",
    "        super(NPLM, self).__init__()\n",
    "        self.C = nn.Embedding(VOCAB_SIZE, EMBED_SIZE).cuda()\n",
    "        self.H = nn.Parameter(torch.randn((NGRAM_SIZE-1)*EMBED_SIZE, HIDDEN_SIZE).type(dtype)).cuda()\n",
    "        self.W = nn.Parameter(torch.randn((NGRAM_SIZE-1)*EMBED_SIZE, VOCAB_SIZE).type(dtype)).cuda()\n",
    "        self.d = nn.Parameter(torch.randn(HIDDEN_SIZE).type(dtype)).cuda()\n",
    "        self.U = nn.Parameter(torch.randn(HIDDEN_SIZE, VOCAB_SIZE).type(dtype)).cuda()\n",
    "        self.b = nn.Parameter(torch.randn(VOCAB_SIZE).type(dtype)).cuda()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.C(X)\n",
    "        X = X.view(-1, n_step * m)\n",
    "        tanh = torch.tanh(self.d + torch.mm(X, self.H))\n",
    "        output = self.b + torch.mm(X, self.W) + torch.mm(tanh, self.U)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NPLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.5323, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output, target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

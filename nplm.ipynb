{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets.txt', 'r', encoding='utf-8') as f:\n",
    "    datasets = f.readlines()\n",
    "    datasets = ''.join(datasets).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "    def __init__(self, target, n=2):\n",
    "        self.sep = ''\n",
    "        if self._is_sentence(target):\n",
    "            target = target.strip().split(' ')\n",
    "            self.sep = ' '\n",
    "        self.text = target\n",
    "        self.n = n\n",
    "    \n",
    "    def _is_sentence(self, target):\n",
    "        if len(target.strip().split(' ')) > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get(self):\n",
    "        # zip 구현은 n만큼 객체가 있어야해서 귀찮음\n",
    "        \"\"\"List Comprehension\"\"\"\n",
    "        return [self.sep.join(self.text[i:i+self.n])\n",
    "                for i in range(len(self.text)-(self.n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['닭 쫓던', '쫓던 개', '개 지붕만', '지붕만 쳐다본다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGram(' '.join(docs[150]), 2).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-106-ed8f598ae42f>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-106-ed8f598ae42f>\"\u001b[1;36m, line \u001b[1;32m50\u001b[0m\n\u001b[1;33m    loss = criterion(output_vec, target_vec)\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NPLM(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Neural Probabilistic Language Model \n",
    "    Implemenataion with Pytorch!!\n",
    "    End to End Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 VOCAB_SIZE, \n",
    "                 FEATURE_VECTOR_SIZE,\n",
    "                 WINDOW_SIZE=2,\n",
    "                 HIDDEN_SIZE=32):\n",
    "        super(NPLM, self).__init__()\n",
    "        self.WINDOW_SIZE = WINDOW_SIZE\n",
    "        self.VOCAB_SIZE = VOCAB_SIZE\n",
    "        self.HIDDEN_SIZE = HIDDEN_SIZE\n",
    "        # projection layer (linear) - lookup\n",
    "        self.proj_layer = nn.Linear(VOCAB_SIZE, FEATURE_VECTOR_SIZE, bias=False)\n",
    "        # Hidden layer\n",
    "        self.hidden_layer = nn.Linear(FEATURE_VECTOR_SIZE * (WINDOW_SIZE-1), \n",
    "                                      HIDDEN_SIZE, bias=True)\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Bilinear(FEATURE_VECTOR_SIZE * (WINDOW_SIZE-1),\n",
    "                                        HIDDEN_SIZE,\n",
    "                                        VOCAB_SIZE,\n",
    "                                        bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # look-up\n",
    "        x = self.proj_layer(x)\n",
    "        # flatten\n",
    "        err_msg = \"입력한 window size와 input의 n이 다릅니다.\"\n",
    "        assert (self.WINDOW_SIZE-1) == x.size(0), err_msg\n",
    "        x = x.flatten()\n",
    "        Hx = torch.tanh(self.hidden_layer(x))\n",
    "        output = torch.softmax(self.output_layer(x, Hx), dim=0)\n",
    "        return output\n",
    "    \n",
    "    def train(self, train_generator):\n",
    "        # Create your optimizer and criterion\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # in your training loop:\n",
    "        optimizer.zero_grad()  # zero the gradient buffers\n",
    "        for input_vec, target_vec in train_generator:\n",
    "            output_vec = self(input_vec)\n",
    "            output_vec = output_vec.reshape([1, output_vec.size(0)]\n",
    "            loss = criterion(output_vec, target_vec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Done.')\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_dataset:\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate N-Gram Datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets, n=2):\n",
    "        self.n = n\n",
    "        self.datasets = datasets\n",
    "        docs = [data.split(' ') for data in datasets]\n",
    "        tokens = [t for doc in docs for t in doc]\n",
    "        uniq_tokens = list(set(tokens))\n",
    "        self.token2ix = {token:vec.argmax() for token, vec in \n",
    "            zip(uniq_tokens, np.eye(len(uniq_tokens)))}\n",
    "        self.ix2token = {vec.argmax():token for token, vec in \n",
    "                    zip(uniq_tokens, np.eye(len(uniq_tokens)))}\n",
    "        self.token2vec = {token:vec for token, vec in \n",
    "                    zip(uniq_tokens, np.eye(len(uniq_tokens)))}\n",
    "        \n",
    "    def get_generator(self):\n",
    "        for doc in self.datasets:\n",
    "            if isinstance(doc, str):\n",
    "                x = doc.strip().split(' ')\n",
    "            x = list(map(lambda t: self.token2vec[t], x))\n",
    "            x = [torch.from_numpy(np.array(x[i:i+self.n])).float()\n",
    "                 for i in range(len(x) - (self.n-1))]\n",
    "            for i in x:\n",
    "                input_vec = i[:-1]\n",
    "                target_vec = torch.LongTensor([i[-1].argmax()])\n",
    "                yield input_vec, target_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NPLM(len(uniq_tokens), 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gen_dataset(datasets, n=3)\n",
    "generator = train_data.get_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object gen_dataset.get_generator at 0x000001F1089D9DB0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NPLM(\n",
       "  (proj_layer): Linear(in_features=1008, out_features=100, bias=False)\n",
       "  (hidden_layer): Linear(in_features=200, out_features=32, bias=True)\n",
       "  (output_layer): Bilinear(in1_features=200, in2_features=32, out_features=1008, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([100, 1008])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0120, -0.0098,  0.0119,  ..., -0.0147,  0.0241,  0.0048],\n",
       "         [-0.0083, -0.0167,  0.0222,  ..., -0.0259, -0.0223, -0.0244],\n",
       "         [ 0.0196, -0.0231, -0.0113,  ...,  0.0133,  0.0118,  0.0214],\n",
       "         ...,\n",
       "         [ 0.0269,  0.0172,  0.0030,  ..., -0.0280, -0.0281, -0.0115],\n",
       "         [-0.0219, -0.0017, -0.0014,  ...,  0.0162,  0.0219,  0.0019],\n",
       "         [-0.0185, -0.0126,  0.0252,  ..., -0.0171,  0.0030, -0.0187]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.0483, -0.0542, -0.0143,  ..., -0.0076, -0.0691, -0.0602],\n",
       "         [-0.0521,  0.0232,  0.0646,  ..., -0.0415, -0.0524,  0.0350],\n",
       "         [-0.0597,  0.0071, -0.0537,  ...,  0.0466, -0.0412,  0.0438],\n",
       "         ...,\n",
       "         [ 0.0552,  0.0435, -0.0378,  ..., -0.0251, -0.0098,  0.0594],\n",
       "         [-0.0499,  0.0446,  0.0160,  ..., -0.0186, -0.0102,  0.0625],\n",
       "         [-0.0525,  0.0234, -0.0160,  ..., -0.0511, -0.0266,  0.0329]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0531, -0.0329, -0.0083,  0.0214,  0.0148,  0.0634,  0.0234,  0.0429,\n",
       "         -0.0104,  0.0100, -0.0661, -0.0370,  0.0575, -0.0266,  0.0012,  0.0604,\n",
       "          0.0647, -0.0018, -0.0203,  0.0242, -0.0430, -0.0156, -0.0641,  0.0091,\n",
       "          0.0391,  0.0041, -0.0559,  0.0657,  0.0628, -0.0249, -0.0150, -0.0399],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[[-0.0131, -0.0160, -0.0007,  ...,  0.0340,  0.0258,  0.0048],\n",
       "          [ 0.0665,  0.0614, -0.0585,  ..., -0.0443,  0.0522, -0.0387],\n",
       "          [ 0.0002, -0.0357,  0.0204,  ...,  0.0169, -0.0440, -0.0045],\n",
       "          ...,\n",
       "          [ 0.0418,  0.0247,  0.0537,  ...,  0.0472,  0.0098, -0.0266],\n",
       "          [ 0.0424, -0.0239, -0.0599,  ...,  0.0005,  0.0073, -0.0639],\n",
       "          [-0.0455, -0.0478,  0.0084,  ..., -0.0203, -0.0201,  0.0678]],\n",
       " \n",
       "         [[-0.0248, -0.0681,  0.0206,  ..., -0.0541, -0.0467, -0.0033],\n",
       "          [ 0.0558, -0.0441, -0.0626,  ..., -0.0086,  0.0298, -0.0226],\n",
       "          [-0.0223, -0.0412, -0.0478,  ...,  0.0278, -0.0345, -0.0544],\n",
       "          ...,\n",
       "          [-0.0091, -0.0253,  0.0611,  ...,  0.0545,  0.0272, -0.0472],\n",
       "          [ 0.0676,  0.0400, -0.0475,  ...,  0.0200,  0.0194, -0.0395],\n",
       "          [ 0.0437,  0.0038, -0.0193,  ..., -0.0269,  0.0172, -0.0430]],\n",
       " \n",
       "         [[-0.0275,  0.0529, -0.0178,  ..., -0.0540,  0.0193, -0.0191],\n",
       "          [ 0.0608,  0.0201,  0.0015,  ..., -0.0466, -0.0171, -0.0596],\n",
       "          [-0.0675, -0.0484,  0.0497,  ...,  0.0700,  0.0226,  0.0464],\n",
       "          ...,\n",
       "          [-0.0117, -0.0019,  0.0030,  ..., -0.0254,  0.0103,  0.0626],\n",
       "          [ 0.0585,  0.0597,  0.0635,  ..., -0.0491, -0.0410, -0.0140],\n",
       "          [ 0.0258, -0.0231,  0.0460,  ..., -0.0200, -0.0642,  0.0556]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0230,  0.0631, -0.0526,  ...,  0.0336, -0.0012,  0.0246],\n",
       "          [ 0.0583,  0.0054,  0.0105,  ...,  0.0674, -0.0600, -0.0572],\n",
       "          [ 0.0138, -0.0109,  0.0232,  ...,  0.0221,  0.0163,  0.0681],\n",
       "          ...,\n",
       "          [-0.0317,  0.0106, -0.0203,  ...,  0.0039, -0.0146, -0.0426],\n",
       "          [ 0.0535,  0.0394, -0.0630,  ..., -0.0703,  0.0587,  0.0138],\n",
       "          [-0.0695, -0.0151,  0.0474,  ..., -0.0342,  0.0302, -0.0218]],\n",
       " \n",
       "         [[-0.0479,  0.0526, -0.0022,  ...,  0.0207, -0.0490, -0.0198],\n",
       "          [-0.0185, -0.0371,  0.0670,  ...,  0.0291, -0.0426, -0.0304],\n",
       "          [ 0.0415,  0.0325, -0.0329,  ...,  0.0699, -0.0341, -0.0536],\n",
       "          ...,\n",
       "          [ 0.0587,  0.0384,  0.0042,  ..., -0.0117, -0.0610,  0.0231],\n",
       "          [-0.0425, -0.0461,  0.0255,  ...,  0.0051, -0.0517,  0.0308],\n",
       "          [-0.0688,  0.0391, -0.0630,  ..., -0.0323,  0.0409,  0.0393]],\n",
       " \n",
       "         [[-0.0474,  0.0314,  0.0194,  ...,  0.0644,  0.0368,  0.0672],\n",
       "          [ 0.0194,  0.0577,  0.0430,  ...,  0.0471, -0.0416,  0.0219],\n",
       "          [ 0.0180,  0.0200, -0.0287,  ...,  0.0662, -0.0199,  0.0695],\n",
       "          ...,\n",
       "          [ 0.0237, -0.0094,  0.0479,  ...,  0.0134,  0.0670, -0.0268],\n",
       "          [ 0.0174,  0.0377, -0.0041,  ..., -0.0533,  0.0123,  0.0195],\n",
       "          [ 0.0328,  0.0149, -0.0095,  ...,  0.0683,  0.0501, -0.0399]]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.0112, -0.0642, -0.0295,  ..., -0.0358, -0.0301, -0.0321],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected device cpu and dtype Float but got device cpu and dtype Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-92b27b1170c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-402ead9a607c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_generator)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minput_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moutput_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultilabel_soft_margin_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\basic\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmultilabel_soft_margin_loss\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2259\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogsigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogsigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cpu and dtype Long"
     ]
    }
   ],
   "source": [
    "net.train(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8982, 0.8050, 0.6393, 0.9983, 0.5731, 0.0469, 0.5560, 0.1476, 0.8404,\n",
       "         0.5544]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0009, 0.0009, 0.0010,  ..., 0.0010, 0.0009, 0.0010]],\n",
       "       grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape([1, a.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9158, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion(a.reshape([1, a.size(0)]), torch.LongTensor([b.argmax()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(669)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(x[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(611)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][-1].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(582)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([634, 550, 259, 465])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0163,  0.0080, -0.0241,  0.0136,  0.0069,  0.0193,  0.0315, -0.0069,\n",
       "          0.0127,  0.0154,  0.0088, -0.0205, -0.0042,  0.0089, -0.0159,  0.0020,\n",
       "          0.0220,  0.0237,  0.0160, -0.0091, -0.0063,  0.0231, -0.0018,  0.0316,\n",
       "         -0.0234,  0.0001, -0.0205, -0.0156,  0.0072,  0.0112, -0.0093, -0.0216,\n",
       "          0.0305, -0.0071,  0.0112,  0.0172, -0.0053,  0.0184, -0.0093,  0.0234,\n",
       "          0.0038,  0.0249,  0.0088,  0.0060, -0.0043,  0.0076,  0.0200, -0.0235,\n",
       "          0.0247, -0.0255, -0.0071, -0.0025,  0.0132, -0.0248, -0.0055,  0.0059,\n",
       "         -0.0270, -0.0022,  0.0309, -0.0256, -0.0132, -0.0182, -0.0303, -0.0048,\n",
       "         -0.0084, -0.0089,  0.0169,  0.0060, -0.0037,  0.0281, -0.0128,  0.0133,\n",
       "         -0.0129,  0.0196, -0.0231, -0.0070,  0.0289,  0.0052,  0.0251, -0.0084,\n",
       "         -0.0255,  0.0271, -0.0269, -0.0199,  0.0065,  0.0041, -0.0147,  0.0312,\n",
       "         -0.0155, -0.0090, -0.0319,  0.0019, -0.0048,  0.0304, -0.0142, -0.0200,\n",
       "         -0.0109, -0.0167,  0.0154,  0.0254],\n",
       "        [-0.0035,  0.0086, -0.0227,  0.0041, -0.0190,  0.0235,  0.0060,  0.0131,\n",
       "         -0.0116, -0.0031, -0.0266,  0.0053,  0.0068, -0.0068,  0.0264, -0.0089,\n",
       "          0.0252, -0.0037, -0.0109,  0.0299,  0.0310, -0.0270,  0.0015, -0.0164,\n",
       "          0.0020, -0.0154,  0.0023, -0.0237, -0.0308,  0.0288, -0.0017, -0.0207,\n",
       "          0.0215,  0.0255,  0.0008, -0.0132,  0.0294,  0.0166, -0.0018,  0.0056,\n",
       "          0.0015,  0.0248, -0.0302, -0.0294,  0.0108, -0.0312, -0.0252, -0.0112,\n",
       "         -0.0019,  0.0157,  0.0196,  0.0303,  0.0235,  0.0310, -0.0285,  0.0114,\n",
       "         -0.0040, -0.0046,  0.0198, -0.0005, -0.0258,  0.0131, -0.0089,  0.0002,\n",
       "         -0.0077,  0.0250, -0.0239, -0.0267, -0.0141, -0.0306, -0.0015, -0.0062,\n",
       "         -0.0205, -0.0155,  0.0237,  0.0059, -0.0217,  0.0029, -0.0097,  0.0041,\n",
       "          0.0261, -0.0258,  0.0083, -0.0038, -0.0313, -0.0302,  0.0065,  0.0014,\n",
       "          0.0256, -0.0320, -0.0021,  0.0122, -0.0156,  0.0042, -0.0126,  0.0204,\n",
       "          0.0088, -0.0042, -0.0282,  0.0271],\n",
       "        [-0.0265,  0.0307,  0.0261, -0.0289, -0.0236,  0.0268, -0.0035,  0.0314,\n",
       "         -0.0238, -0.0317,  0.0115,  0.0074, -0.0307, -0.0043,  0.0190, -0.0181,\n",
       "          0.0103, -0.0026, -0.0052,  0.0023,  0.0109, -0.0272,  0.0301, -0.0253,\n",
       "          0.0288, -0.0250,  0.0096,  0.0050, -0.0082,  0.0070,  0.0015, -0.0273,\n",
       "         -0.0262,  0.0074,  0.0115,  0.0136,  0.0018,  0.0214, -0.0152,  0.0167,\n",
       "          0.0185, -0.0049, -0.0278, -0.0196,  0.0174, -0.0164,  0.0168,  0.0184,\n",
       "         -0.0221,  0.0308,  0.0213,  0.0047,  0.0285,  0.0290, -0.0071,  0.0053,\n",
       "          0.0269,  0.0162, -0.0065,  0.0047, -0.0051,  0.0038,  0.0155,  0.0186,\n",
       "         -0.0312, -0.0076,  0.0244,  0.0146, -0.0059,  0.0126, -0.0197, -0.0081,\n",
       "          0.0103,  0.0005, -0.0157,  0.0138, -0.0299,  0.0130, -0.0033,  0.0258,\n",
       "         -0.0316,  0.0138,  0.0024,  0.0160,  0.0245, -0.0018,  0.0298, -0.0073,\n",
       "          0.0117, -0.0085,  0.0299, -0.0211,  0.0294, -0.0091, -0.0076, -0.0058,\n",
       "          0.0145,  0.0168,  0.0212,  0.0287],\n",
       "        [-0.0290, -0.0080,  0.0185,  0.0195,  0.0093,  0.0221, -0.0062,  0.0102,\n",
       "          0.0282, -0.0142,  0.0133, -0.0232, -0.0111,  0.0113,  0.0244,  0.0032,\n",
       "          0.0113, -0.0112,  0.0244, -0.0280, -0.0091,  0.0033,  0.0110, -0.0041,\n",
       "          0.0169, -0.0246, -0.0050,  0.0133, -0.0191,  0.0101, -0.0036,  0.0310,\n",
       "          0.0026, -0.0062, -0.0065, -0.0173,  0.0065,  0.0079,  0.0118,  0.0060,\n",
       "         -0.0256,  0.0272,  0.0271, -0.0318,  0.0083, -0.0197,  0.0199,  0.0129,\n",
       "          0.0306, -0.0014,  0.0012, -0.0198, -0.0031, -0.0315,  0.0003, -0.0132,\n",
       "         -0.0056,  0.0120,  0.0269, -0.0107,  0.0055, -0.0197, -0.0219, -0.0172,\n",
       "          0.0194, -0.0264,  0.0224,  0.0120, -0.0165, -0.0007,  0.0017, -0.0148,\n",
       "          0.0082, -0.0310, -0.0060,  0.0090, -0.0028,  0.0280,  0.0249, -0.0195,\n",
       "          0.0123,  0.0098, -0.0080, -0.0173, -0.0226, -0.0292,  0.0259, -0.0049,\n",
       "          0.0095,  0.0154,  0.0117,  0.0252, -0.0105,  0.0286,  0.0187,  0.0060,\n",
       "          0.0009, -0.0228,  0.0270, -0.0173]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[0].data[:, (634, 550, 259, 465)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0163,  0.0080, -0.0241,  0.0136,  0.0069,  0.0193,  0.0315, -0.0069,\n",
       "          0.0127,  0.0154,  0.0088, -0.0205, -0.0042,  0.0089, -0.0159,  0.0020,\n",
       "          0.0220,  0.0237,  0.0160, -0.0091, -0.0063,  0.0231, -0.0018,  0.0316,\n",
       "         -0.0234,  0.0001, -0.0205, -0.0156,  0.0072,  0.0112, -0.0093, -0.0216,\n",
       "          0.0305, -0.0071,  0.0112,  0.0172, -0.0053,  0.0184, -0.0093,  0.0234,\n",
       "          0.0038,  0.0249,  0.0088,  0.0060, -0.0043,  0.0076,  0.0200, -0.0235,\n",
       "          0.0247, -0.0255, -0.0071, -0.0025,  0.0132, -0.0248, -0.0055,  0.0059,\n",
       "         -0.0270, -0.0022,  0.0309, -0.0256, -0.0132, -0.0182, -0.0303, -0.0048,\n",
       "         -0.0084, -0.0089,  0.0169,  0.0060, -0.0037,  0.0281, -0.0128,  0.0133,\n",
       "         -0.0129,  0.0196, -0.0231, -0.0070,  0.0289,  0.0052,  0.0251, -0.0084,\n",
       "         -0.0255,  0.0271, -0.0269, -0.0199,  0.0065,  0.0041, -0.0147,  0.0312,\n",
       "         -0.0155, -0.0090, -0.0319,  0.0019, -0.0048,  0.0304, -0.0142, -0.0200,\n",
       "         -0.0109, -0.0167,  0.0154,  0.0254], grad_fn=<SelectBackward>),\n",
       " tensor([-0.0035,  0.0086, -0.0227,  0.0041, -0.0190,  0.0235,  0.0060,  0.0131,\n",
       "         -0.0116, -0.0031, -0.0266,  0.0053,  0.0068, -0.0068,  0.0264, -0.0089,\n",
       "          0.0252, -0.0037, -0.0109,  0.0299,  0.0310, -0.0270,  0.0015, -0.0164,\n",
       "          0.0020, -0.0154,  0.0023, -0.0237, -0.0308,  0.0288, -0.0017, -0.0207,\n",
       "          0.0215,  0.0255,  0.0008, -0.0132,  0.0294,  0.0166, -0.0018,  0.0056,\n",
       "          0.0015,  0.0248, -0.0302, -0.0294,  0.0108, -0.0312, -0.0252, -0.0112,\n",
       "         -0.0019,  0.0157,  0.0196,  0.0303,  0.0235,  0.0310, -0.0285,  0.0114,\n",
       "         -0.0040, -0.0046,  0.0198, -0.0005, -0.0258,  0.0131, -0.0089,  0.0002,\n",
       "         -0.0077,  0.0250, -0.0239, -0.0267, -0.0141, -0.0306, -0.0015, -0.0062,\n",
       "         -0.0205, -0.0155,  0.0237,  0.0059, -0.0217,  0.0029, -0.0097,  0.0041,\n",
       "          0.0261, -0.0258,  0.0083, -0.0038, -0.0313, -0.0302,  0.0065,  0.0014,\n",
       "          0.0256, -0.0320, -0.0021,  0.0122, -0.0156,  0.0042, -0.0126,  0.0204,\n",
       "          0.0088, -0.0042, -0.0282,  0.0271], grad_fn=<SelectBackward>),\n",
       " tensor([-0.0265,  0.0307,  0.0261, -0.0289, -0.0236,  0.0268, -0.0035,  0.0314,\n",
       "         -0.0238, -0.0317,  0.0115,  0.0074, -0.0307, -0.0043,  0.0190, -0.0181,\n",
       "          0.0103, -0.0026, -0.0052,  0.0023,  0.0109, -0.0272,  0.0301, -0.0253,\n",
       "          0.0288, -0.0250,  0.0096,  0.0050, -0.0082,  0.0070,  0.0015, -0.0273,\n",
       "         -0.0262,  0.0074,  0.0115,  0.0136,  0.0018,  0.0214, -0.0152,  0.0167,\n",
       "          0.0185, -0.0049, -0.0278, -0.0196,  0.0174, -0.0164,  0.0168,  0.0184,\n",
       "         -0.0221,  0.0308,  0.0213,  0.0047,  0.0285,  0.0290, -0.0071,  0.0053,\n",
       "          0.0269,  0.0162, -0.0065,  0.0047, -0.0051,  0.0038,  0.0155,  0.0186,\n",
       "         -0.0312, -0.0076,  0.0244,  0.0146, -0.0059,  0.0126, -0.0197, -0.0081,\n",
       "          0.0103,  0.0005, -0.0157,  0.0138, -0.0299,  0.0130, -0.0033,  0.0258,\n",
       "         -0.0316,  0.0138,  0.0024,  0.0160,  0.0245, -0.0018,  0.0298, -0.0073,\n",
       "          0.0117, -0.0085,  0.0299, -0.0211,  0.0294, -0.0091, -0.0076, -0.0058,\n",
       "          0.0145,  0.0168,  0.0212,  0.0287], grad_fn=<SelectBackward>),\n",
       " tensor([-0.0290, -0.0080,  0.0185,  0.0195,  0.0093,  0.0221, -0.0062,  0.0102,\n",
       "          0.0282, -0.0142,  0.0133, -0.0232, -0.0111,  0.0113,  0.0244,  0.0032,\n",
       "          0.0113, -0.0112,  0.0244, -0.0280, -0.0091,  0.0033,  0.0110, -0.0041,\n",
       "          0.0169, -0.0246, -0.0050,  0.0133, -0.0191,  0.0101, -0.0036,  0.0310,\n",
       "          0.0026, -0.0062, -0.0065, -0.0173,  0.0065,  0.0079,  0.0118,  0.0060,\n",
       "         -0.0256,  0.0272,  0.0271, -0.0318,  0.0083, -0.0197,  0.0199,  0.0129,\n",
       "          0.0306, -0.0014,  0.0012, -0.0198, -0.0031, -0.0315,  0.0003, -0.0132,\n",
       "         -0.0056,  0.0120,  0.0269, -0.0107,  0.0055, -0.0197, -0.0219, -0.0172,\n",
       "          0.0194, -0.0264,  0.0224,  0.0120, -0.0165, -0.0007,  0.0017, -0.0148,\n",
       "          0.0082, -0.0310, -0.0060,  0.0090, -0.0028,  0.0280,  0.0249, -0.0195,\n",
       "          0.0123,  0.0098, -0.0080, -0.0173, -0.0226, -0.0292,  0.0259, -0.0049,\n",
       "          0.0095,  0.0154,  0.0117,  0.0252, -0.0105,  0.0286,  0.0187,  0.0060,\n",
       "          0.0009, -0.0228,  0.0270, -0.0173], grad_fn=<SelectBackward>)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0146, -0.0159,  0.0211,  ...,  0.0264,  0.0148,  0.0240],\n",
       "         [-0.0049, -0.0193, -0.0209,  ..., -0.0087, -0.0274, -0.0091],\n",
       "         [ 0.0309,  0.0227, -0.0100,  ..., -0.0041,  0.0278, -0.0281],\n",
       "         ...,\n",
       "         [ 0.0066,  0.0188,  0.0010,  ..., -0.0053,  0.0271,  0.0142],\n",
       "         [-0.0197,  0.0185,  0.0274,  ...,  0.0155,  0.0085, -0.0012],\n",
       "         [ 0.0251, -0.0215, -0.0268,  ...,  0.0038,  0.0298,  0.0173]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[-0.0759,  0.0057,  0.0638,  ...,  0.0636, -0.0993,  0.0426],\n",
       "         [-0.0357,  0.0384, -0.0809,  ...,  0.0799, -0.0884,  0.0217],\n",
       "         [-0.0756,  0.0711, -0.0133,  ...,  0.0607,  0.0302, -0.0936],\n",
       "         ...,\n",
       "         [-0.0474, -0.0217, -0.0249,  ...,  0.0639,  0.0803, -0.0315],\n",
       "         [ 0.0881,  0.0205,  0.0734,  ..., -0.0998, -0.0073,  0.0608],\n",
       "         [ 0.0960, -0.0905,  0.0543,  ...,  0.0480,  0.0858,  0.0599]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-0.0669, -0.0079, -0.0907,  0.0049,  0.0653,  0.0796, -0.0882, -0.0651,\n",
       "         -0.0207, -0.0281,  0.0140,  0.0377,  0.0501,  0.0759,  0.0961,  0.0943,\n",
       "         -0.0411,  0.0943, -0.0258, -0.0997, -0.0247, -0.0448, -0.0758,  0.0278,\n",
       "         -0.0628, -0.0364, -0.0549,  0.0162,  0.0717, -0.0426, -0.0506, -0.0434],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([[ 0.1343, -0.0690,  0.1357,  ...,  0.0314, -0.0640,  0.0398],\n",
       "         [-0.1545, -0.0628,  0.1179,  ...,  0.0625, -0.1149, -0.1463],\n",
       "         [-0.1530,  0.0430, -0.0364,  ..., -0.0843,  0.0339,  0.0606],\n",
       "         ...,\n",
       "         [ 0.0054,  0.1744,  0.0266,  ...,  0.1483, -0.0810,  0.0501],\n",
       "         [-0.1099, -0.0902,  0.1526,  ...,  0.0117,  0.0882, -0.0867],\n",
       "         [-0.0889, -0.0314, -0.1093,  ..., -0.0781, -0.0323,  0.1659]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-1.2566e-02, -1.4103e-01, -1.2687e-01,  ..., -1.1272e-01,\n",
       "          8.3476e-05, -5.5280e-02], requires_grad=True)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([781, 781, 781, 781, 781])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(outputs, dim=1).argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
